EPI_data
EPI_data
EPI_data <- read.csv("2010EPI_data.csv")
setwd("C:Users/vogeld2")
getwd()
library(rpart)
library(rpart.plot)
iris
dim(iris)
s_iris <- sample(150,100)
s_iris
iris_train <- iris[s_iris,]
iris_test <- iris[-s_iris,]
dim(iris_test)
dim(iris_train)
decisionTreeModel <- rpart(Species~., iris_train, method = "class")
decisionTreeModel
rpart.plot(decisionTreeModel)
library(rpart.plot)
install.packages('rpart.plot')
library(rpart.plot)
rpart.plot(decisionTreeModel)
setwd("C:/Users/vogeld2/Desktop/Data Analytics/Labs/DataAnalyticsFall2022_Dean_Vogel/lab3")
set.seed(12345)
help(par)
par(mar = rep(0.2,4))
data_Matrix <- matrix(rnorm(400),nrow=40)
image(1:10, 1:40, t(data_Matrix)[,nrow(data_Matrix):1])
heatmap(data_Matrix)
set.seed(678910)
for(i in 1:40){
# flipping a coin and getting the data
coin_Flip <- rbinom(1, size = 1, prob = 0.5)
# if the coin is "Heads", add a common pattern to that row,
if(coin_Flip){
data_Matrix[i, ] <- data_Matrix[i, ] + rep(c(0,3), each =5)
}
}
library(gdata)
par(mar= rep(0.2, 4))
image(1:10, 1:40, t(data_Matrix)[, nrow(data_Matrix):1])
par(mar=rep(0.2, 4))
heatmap(data_Matrix)
hh <- hclust(dist(data_Matrix))
data_Matrix_Ordered <- data_Matrix[hh$order,]
par(mfrow = c(1,3))
image(t(data_Matrix_Ordered)[, nrow(data_Matrix_Ordered):1])
plot(rowMeans(data_Matrix_Ordered),40:1, xlab = "The Row Mean", ylab = "Row", pch = 19)
plot(colMeans(data_Matrix_Ordered),40:1, xlab = "Column", ylab = "Column Mean", pch = 19)
plot(colMeans(data_Matrix_Ordered), xlab = "Column", ylab = "Column Mean", pch = 19)
pairs(~ Fertility + Education + Catholic, data = swiss, subset = Education < 20, main = "Swiss data, Education < 20")
require(party)
swiss_ctree <- ctree(Fertility ~ Agriculture + Education + Catholic, data = swiss)
require(party)
swiss_ctree <- ctree(Fertility ~ Agriculture + Education + Catholic, data = swiss)
plot(swiss_ctree)
require(kknn)
install.packages("kknn")
require(kknn)
data(iris)
m <- dim(iris)[1]
val <- sample(1:m, size = round(m/3), replace = FALSE,
prob = rep(1/m, m))
iris.learn <- iris[-val,]
iris.valid <- iris[val,]
iris.kknn <- kknn(Species~., iris.learn, iris.valid, distance = 1,
kernel = "triangular")
summary(iris.kknn)
fit <- fitted(iris.kknn)
table(iris.valid$Species, fit)
pcol <- as.character(as.numeric(iris.valid$Species))
pairs(iris.valid[1:4], pch = pcol, col = c("green3", "red")[(iris.valid$Species != fit)+1])
require(kknn)
data(ionosphere)
ionosphere.learn <- ionosphere[1:200,]
ionosphere.valid <- ionosphere[-c(1:200),]
fit.kknn <- kknn(class ~ ., ionosphere.learn, ionosphere.valid)
table(ionosphere.valid$class, fit.kknn$fit)
(fit.train1 <- train.kknn(class ~ ., ionosphere.learn, kmax = 15,
kernel = c("triangular", "rectangular", "epanechnikov", "optimal"), distance = 1))
table(predict(fit.train1, ionosphere.valid), ionosphere.valid$class)
(fit.train2 <- train.kknn(class ~ ., ionosphere.learn, kmax = 15,
kernel = c("triangular", "rectangular", "epanechnikov", "optimal"), distance = 2))
table(predict(fit.train2, ionosphere.valid), ionosphere.valid$class)
data(swiss)
pairs(~ Fertility + Education + Catholic, data = swiss, subset = Education < 20, main = "Swiss data, Education < 20")
data(swiss)
sclass <- kmeans(swiss[2:6], 3)
table(sclass$cluster, swiss[,1])
nyt1<-read.csv("nyt1.csv")
setwd("C:/Users/vogeld2/Desktop/Data Analytics/group2")
nyt1<-read.csv("nyt1.csv")
nyt1<-nyt1[which(nyt1$Impressions>0 & nyt1$Clicks>0 & nyt1$Age>0),]
nnyt1<-dim(nyt1)[1]		# shrink it down!
sampling.rate=0.9
num.test.set.labels=nnyt1*(1.-sampling.rate)
training <-sample(1:nnyt1,sampling.rate*nnyt1, replace=FALSE)
train<-subset(nyt1[training,],select=c(Age,Impressions))
testing<-setdiff(1:nnyt1,training)
test<-subset(nyt1[testing,],select=c(Age,Impressions))
cg<-nyt1$Gender[training]
true.labels<-nyt1$Gender[testing]
classif<-knn(train,test,cg,k=5) #
classif
library("class")
classif<-knn(train,test,cg,k=5) #
classif
attributes(.Last.value)
require(rpart)
Swiss_rpart <- rpart(Fertility ~ Agriculture + Education + Catholic, data = swiss)
plot(swiss_rpart) # try some different plot options
plot(Swiss_rpart) # try some different plot options
text(Swiss_rpart) # try some different text options
require(party)
treeSwiss<-ctree(Species ~ ., data=iris)
plot(treeSwiss)
cforest(Species ~ ., data=iris, controls=cforest_control(mtry=2, mincriterion=0))
treeFert<-ctree(Fertility ~ Agriculture + Education + Catholic, data = swiss)
cforest(Fertility ~ Agriculture + Education + Catholic, data = swiss, controls=cforest_control(mtry=2, mincriterion=0))
library(tree)
tr <- tree(Species ~ ., data=iris)
tr
tr$frame
plot(tr)
text(tr)
# look at help info, vary parameters.
dev.off()
tr <- tree(Species ~ ., data=iris)
tr
tr$frame
plot(tr)
text(tr)
help(plot)
plot(tr, asp = 1.5)
text(tr)
plot(tr, asp = 4)
text(tr)
plot(tr, asp = 10)
text(tr)
plot(tr, asp = 0.5)
text(tr)
text(tr,pretty = 1)
plot(tr, asp = 0.5, pretty(tr))
plot(tr, asp = 0.5, pretty = 1)
# Conditional Inference Tree for Mileage
fit2M <- ctree(Mileage~Price + Country + Reliability + Type, data=na.omit(cu.summary))
summary(fit2M)
# plot tree
plot(fit2M, uniform=TRUE, main="CI Tree Tree for Mileage ")
text(fit2M, use.n=TRUE, all=TRUE, cex=.8)
# Conditional Inference Tree for Mileage
install.packages("arulesViz")
library("arulesViz")
fit2M <- ctree(Mileage~Price + Country + Reliability + Type, data=na.omit(cu.summary))
summary(fit2M)
# plot tree
plot(fit2M, uniform=TRUE, main="CI Tree Tree for Mileage ")
text(fit2M, use.n=TRUE, all=TRUE, cex=.8)
# Conditional Inference Tree for Mileage
fit2M <- ctree(Mileage~Price + Country + Reliability + Type, data=na.omit(cu.summary))
require(rpart)
Swiss_rpart <- rpart(Fertility ~ Agriculture + Education + Catholic, data = swiss)
plot(Swiss_rpart) # try some different plot options
text(Swiss_rpart) # try some different text options
require(party)
treeSwiss<-ctree(Species ~ ., data=iris)
plot(treeSwiss)
cforest(Species ~ ., data=iris, controls=cforest_control(mtry=2, mincriterion=0))
treeFert<-ctree(Fertility ~ Agriculture + Education + Catholic, data = swiss)
cforest(Fertility ~ Agriculture + Education + Catholic, data = swiss, controls=cforest_control(mtry=2, mincriterion=0))
# look at help info, vary parameters.
dev.off()
library(tree)
tr <- tree(Species ~ ., data=iris)
tr
tr$frame
help(plot)
plot(tr, asp = 0.5)
text(tr,pretty = 1)
# Conditional Inference Tree for Mileage
fit2M <- ctree(Mileage~Price + Country + Reliability + Type, data=na.omit(cu.summary))
summary(fit2M)
# plot tree
plot(fit2M, uniform=TRUE, main="CI Tree Tree for Mileage ")
text(fit2M, use.n=TRUE, all=TRUE, cex=.8)
fitK <- ctree(Kyphosis ~ Age + Number + Start, data=kyphosis)
plot(fitK, main="Conditional Inference Tree for Kyphosis")
plot(fitK, main="Conditional Inference Tree for Kyphosis",type="simple")
data(Titanic)
mdl <- naiveBayes(Survived ~ ., data = Titanic)
mdl
require(rpart)
head(Titanic)
View(Titanic)
Titan_rpart = rpart(Survived ~ Class + Sex + Freq, data= Titanic)
Titan_rpart
Titan_rpart = rpart(Survived ~., data= Titanic)
Titan_rpart
plot(Titan_rpart)
text(Titan_rpart)
plot(Titan_rpart)
text(Titan_rpart)
text(Titan_rpart,use.n=TRUE, all=TRUE, cex=.8)
plot(Titan_rpart,margin = 0.5)
text(Titan_rpart,use.n=TRUE, all=TRUE, cex=.8)
library("titanic")
install.packages("titanic")
library("titanic")
head(titanic)
head(titanic_train)
Titan_rpart = rpart(Survived ~., data= titanic)
head(titanic_gender_class_model)
View(titanic_gender_class_model)
Titan_rpart = rpart(Survived ~., data= titanic_train)
Titan_rpart
plot(Titan_rpart,margin = 0.5)
text(Titan_rpart,use.n=TRUE, all=TRUE, cex=.8)
Titan_rpart = rpart(Survived ~., data= Titanic)
Titan_rpart
Titan_rpart = rpart(Survived ~., data= Titanic)
Titan_rpart
plot(Titan_rpart,margin = 0.5)
text(Titan_rpart,use.n=TRUE, all=TRUE, cex=.8)
tree_Titan<-ctree(Survived ~., data=Titanic)
plot(tree_Titan)
titan_h = hclust(as.matrix(Titanic))
titan_h = hclust(dist(as.matrix(Titanic)))
titan_h
Image(titan_h)
plot(titan_h)
View(Poland)
View(poland)
setwd("C:/Users/vogeld2/Desktop/Data Analytics/Labs/DataAnalyticsFall2022_Dean_Vogel/lab4")
cars1 <- cars[1:30,]
head(cars1)
cars_outliers <- data.frame(speed=c(19,19,20,20,20), dist=c(190,186,210,220,218))
head(cars_outliers)
cars2 <- rbind(cars1, cars_outliers)
par(mfrow = c(1,2))
plot(cars2$speed, cars2$dist, xlim=c(0, 28), ylim=c(0, 230), main="With Outliers", xlab="speed", ylab="dist",
pch="*", col="red", cex=2)
abline(lm(dist ~ speed, data=cars2), col="blue", lwd=3, lty=2)
plot(cars1$speed, cars1$dist, xlim=c(0, 28), ylim=c(0, 230), main="Outliers removed \n A much better fit!",
xlab="speed", ylab="dist", pch="*", col="red", cex=2)
abline(lm(dist ~ speed, data=cars1), col="blue", lwd=3, lty=2)
# ISLR Package (ISRL 7th Eidtion Textbook )
# KMeans example with Iris dataset
# KMeans is an Unsupervised technique
library(ISLR)
head(iris)
str(iris)
# visually inspecting the three factors: Species clusters with a plot
plot1 <- ggplot(iris, aes(Petal.Length,Petal.Width, color=Species))
print(plot1 + geom_point(size=3))
# KNN example using ISLR package (Textbook)
library(ISLR) # you need to install the ISLR package first
# Caravan dataset is about the insurance
library(class)
head(Caravan)
str(Caravan)
# Purchase: Factor w/ 2 levels "No","Yes": 1 1 1 1 1 1 1 1 1 1 ...
# "Yes" or "No" indicates wheater people purchased the insurance policy or not.
dim(Caravan) # it has 5822 rows (observations) and 86 features/columns
summary(Caravan)
summary(Caravan$Purchase)
# Yes: 348 people purchased the insurance and No:5474 people did not purchase the insurance
# check for any NA and missing values
any(is.na(Caravan)) # FALSE, that means no NA values in this dataset
# Now we want to check the scales of the variables becuase the variable with a large scale
# will have a larger effect on distances between observations when using the KNN Algorithm
# Checking the Variances on features/Colums using the var() function in R
var(Caravan[,1]) # Variance of the 1st column is 165.0378
var(Caravan[,2]) # Variance of the 2nd column is 0.1647
var(Caravan[,3]) # Variance of the 3rd column is 0.6238
# You can see that the variances are different for each of the column variables, 1st one is 165.03 and 2nd one is 0.16
# and there is a huge difference, because of that we want to standardize the variables.
# We will do that for all the columns except the "Purchase" variable which we are going predict.
purchase <- Caravan[,86] # you can write the same as
# purchase <-Caravan[,'Purchase'] with the column name, we use the column number 86 for the simplcity.
purchase
# Now we want to Standardize the columns except the 86th column
StandardizedCaravan <- scale(Caravan[,-86]) # when we use -86 it will not include the 86th column.
var(StandardizedCaravan[,1])
var(StandardizedCaravan[,2])
var(StandardizedCaravan[,3])
# test set
test_index <- 1:1000
test_data <- StandardizedCaravan[test_index,]
test_purchase <- purchase[test_index]
# train set
train_data <- StandardizedCaravan[-test_index,]
train_purchase <- purchase[-test_index]
# set seed
set.seed(101)
predicted_purchase <- knn(train_data,test_data,train_purchase, k = 10)
head(predicted_purchase)
missClassError <- mean(test_purchase != predicted_purchase)
print(missClassError)
# Choosing the K value
# we can write a for-loop
predicted_purchase <- NULL
error_rate <- NULL
for (i in 1:20) {
set.seed(101)
predicted_purchase <- knn(train_data, test_data, train_purchase, k =i)
error_rate[i] <- mean(test_purchase != predicted_purchase)
}
print(error_rate)
# Plot the K value on a graph
library(ggplot2)
k_values <- 1:20
error_df <- data.frame(error_rate, k_values)
print(error_df)
ggplot(error_df,aes(k_values,error_rate)) + geom_point() + geom_line(lty='dotted', color='blue')
# ISLR Package (ISRL 7th Eidtion Textbook )
# KMeans example with Iris dataset
# KMeans is an Unsupervised technique
library(ISLR)
head(iris)
str(iris)
# visually inspecting the three factors: Species clusters with a plot
plot1 <- ggplot(iris, aes(Petal.Length,Petal.Width, color=Species))
print(plot1 + geom_point(size=3))
# set seed for the random numbers
set.seed(101)
# Read the documentation for KMeans in R-Studio
help("kmeans")
irisClusters <- kmeans(iris[,1:4], 3, nstart = 20) # nstart is the number of random start
print(irisClusters)
table(irisClusters$cluster,iris$Species)
# plotting the clusters
library(cluster)
help("clusplot")
clusplot(iris,irisClusters$cluster, color = TRUE, shade = TRUE, labels = 0, lines = 0)
library(cvms)
library(tibble)   # tibble()
set.seed(1)
d_binomial <- tibble("target" = rbinom(100, 1, 0.7),"prediction" = rbinom(100, 1, 0.6))
d_binomial
basic_table <- table(d_binomial)
basic_table
cfm <- as_tibble(basic_table)
cfm
plot_confusion_matrix(cfm,
target_col = "target",
prediction_col = "prediction",
counts_col = "n")
eval <- evaluate(d_binomial,
target_col = "target",
prediction_cols = "prediction",
type = "binomial")
eval
conf_mat <- eval$`Confusion Matrix`[[1]]
conf_mat
plot_confusion_matrix(conf_mat)
